{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab03\n",
    "======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Context\n",
    "#### Neural Network\n",
    "   + NN\n",
    "   + Deep-Feedforward NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Perceptron\n",
    "\n",
    "+ 생물학의 신경망(뉴런)을 묘사한 예측 모델\n",
    "\n",
    "각 뉴런은 다른 뉴런이 출력한 결과를 입력 받아 특정 연산을 수행하고, 계산 결과가 임계치를 넘으면 활성화되어 다음 뉴런에 값을 전달하고, 넘지 않으면 활성화되지 않아 다음 뉴런에 값을 전달하지 않는다.\n",
    "\n",
    " ![Perceptron](./Images/Perceptron.png)\n",
    "\n",
    "(input) x<sub>0</sub>, x<sub>1</sub>, x<sub>2</sub> $\\cdot\\cdot\\cdot$: 입력되는 뉴런의 축삭돌기로부터 전달되는 신호의 양\n",
    "\n",
    "w<sub>0</sub>, w<sub>1</sub>, w<sub>2</sub> $\\cdot\\cdot\\cdot$: 시냅스의 강도, 즉 입력되는 뉴런의 영향력을 나타냅니다.\n",
    "\n",
    "w<sub>0</sub>x<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> $\\cdot\\cdot\\cdot$: 입력되는 신호의 양과 해당 신호의 시냅스 강도가 곱해진 값의 합계\n",
    "\n",
    "f : 최종 합계가 다른 뉴런에게 전달되는 신호의 양을 결정짓는 규칙, 이를 활성화 함수라고 부릅니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network (NN)\n",
    "뉴럴 네트워크란, 여러개의 퍼셉트론으로 이루어진 모델을 이야기 합니다. <br> \n",
    "일반적으로 MLP(Multilayer Neural Network)는 입력 - 은닉 - 출력 3가지 층(Layer)으로 나누어진 뉴럴 네트워크의 기본 모델입니다. <br>\n",
    "기존 하나의 퍼셉트론으로 해결하지 못했던 XOR 문제(비선형 문제)를 해결할 수 있는 모델로 소개됩니다.<br>\n",
    "뉴럴 네트워크는 대부분 지도학습의 일종으로 모델을 학습하여 문제를 해결합니다. 학습단계는 크게 2단계로 나눌 수 있습니다.<br>\n",
    "\n",
    "\n",
    "#### Feedforword 단계\n",
    "들어온 입력을 해당 층의 가중치(w)와 벡터 연산하고, 활성화 함수를 통과시켜 다음 층으로 전달하는 단계<br>\n",
    "앞 -> 뒤 방향으로 입력(Feed)이 이동하기 때문에 Feedforword 라고 부릅니다.\n",
    "![Feedforword](./Images/Feedforword.png)\n",
    "#### Backpropagation 단계\n",
    "출력 층에서 발생하는 오류를 최소화 하기 위해 이전의 층에 오차를 줄이는 방향으로 가중치를 전달하는 단계<br>\n",
    "뒤 -> 앞 방향으로 오차를 최소화하는 역전파(Back Propagation)가 이동하기 때문에 Back Propagation 이라고 부릅니다.\n",
    "![Backpropagarion](./Images/Backpropagation.png)\n",
    "\n",
    "### 어떻게 뉴럴 네트워크가 비선형 문제를 해결할 수 있을까?\n",
    "각각의 퍼셉트론은 선형 분리만을 해결할 수 있는 모델입니다. <br> \n",
    "하지만 이런 선형 분리를 할 수 있는 모델을 여러개를 모아 비선형 분리를 수행하는 것이 뉴럴 네트워크 입니다.<br>\n",
    "![HowNNSolve](./Images/HowNNSolve.png)\n",
    "아래 그림을 보면 4개의 벡터 공간을 선형 분리하는 퍼셉트론들이 하나의 비선형 공간을 분류할 수 있는 벡터 공간을 형성하는 것을 확인할 수 있습니다.<br>\n",
    "직관적으로는 이해하기 어려우시겠지만, 우리가 케익을 4개의 퍼셉트론들이 분할하는 대로 잘라 가운데 부분을 남기는 것을 생각해보시면 이해가 편하실겁니다.\n",
    "\n",
    "### 주의해야할 점\n",
    "뉴럴 네트워크에서 가장 접하기 쉬운 문제로는 훈련 데이터에 모델이 과적합(overfitting)되는 것이 있습니다.<br>\n",
    "과적합은 모델이 너무 복잡하거나, 데이터가 편향된 경우 자주 발생합니다. <br>\n",
    "과적합을 피하는 방법에 대한 내용은 다음 머신러닝 심화 교육때 자세히 배워보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "### 특징\n",
    "Keras는 Tensorflow 와 같은 파이썬 딥러닝 라이브러리 중 하나입니다.<br>\n",
    "Tensorflow는 낮은 수준(Low Level)의 모델링이 가능하다는 장점이 있지만, 상대적으로 쉽게 익히기 어렵다는 단점이 있습니다.<br>\n",
    "하지만, Keras는 유저 친화적인 사용법으로 누구나 쉽게 사용할 수 있는 장점이 있습니다.<br>\n",
    "\n",
    "### Tensorflow와의 합병\n",
    "Tensorflow 1.1에서 contrib.keras으로서 Keras 모듈을 사용할 수 있으며, <br>최종적으로 Tensorflow 1.2에서 tf.keras로 사용할 수 있게 되었습니다.<br>\n",
    "\n",
    "### 최근 Keras와 Tensorflow의 관계\n",
    "Tensorflow 2.0 부터 정식적으로 Tensorflow는 Low Level API로 Keras는 High Level API로 지원됩니다.<br>\n",
    "이번 실습에서는 Keras를 사용해 실습을 진행하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 살펴보기\n",
    "이번에 사용할 데이터는 손글씨 데이터로 유명한 MNIST 데이터 입니다.<br>\n",
    "MNIST(Modified National Institute of Standards and Technology database)는 손으로 쓴 숫자들로 이루어진 대형 데이터셋 입니다.<br>\n",
    "일반적으로 다양한 영상 처리 시스템을 트레이닝하기 위해 사용됩니다. 또한, 기계 학습 분야의 학습 및 검증에 널리 사용된다. <br>\n",
    "MNIST 데이터셋은 28x28 픽셀의 흑백(Gray scale) 이미지입니다.<br>\n",
    "\n",
    "![MNIST](./Images/MNIST.png)\n",
    "\n",
    "data 디렉토리에 준비된 데이터와 라벨을 가져옵니다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.load(join('data','MNIST.npy'))\n",
    "labels = np.load(join('data','Label.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img=plt.imshow(X[1].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 간단한 전처리\n",
    "MNIST 데이터는 28*28에 픽셀마다 0~255 범위의 값을 가진 행렬입니다. <br>\n",
    "각 픽셀에 대해 0~1 사이의 값을 갖도록 255로 나누어줍니다.<br>\n",
    "그리고 네트워크에 데이터를 입력하기 편하게 하도록 2차원 이미지를 벡터형태로 펼치겠습니다.<br>\n",
    "numpy.reshape() 함수를 통해 행렬의 차원을 변경할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X/255\n",
    "X = X.reshape(len(X),784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "값의 범위가 0과 1사이로 잘 되었는지 확인해봅시다.<br>\n",
    "numpy.max(), numpy.min() 으로 확인해 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('최대 : {}, 최소 : {}'.format(np.max(X), np.min(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지들의 정답을 나타내는 label 데이터는 0~9의 값을 가지고 있습니다.<br>\n",
    "해당 label 데이터도 독립적인 벡터 형태로 표현하겠습니다.\n",
    "\n",
    "### 왜 벡터를 사용하나요\n",
    "여기에서 Label 데이터를 그대로 숫자로 표현해도 되지만, 벡터로 표현한 것은 숫자 이미지라는 특성이 수치적으로 1씩 증가하는 관계가 아니기 때문에<br>\n",
    "서로 독립적인 관계를 갖는 벡터로 표현해주는 것이 더 적절합니다. 이러한 기법을 One-hot encoding이라 합니다.<br>\n",
    "Keras에서는 연속 정수형을 벡터형태로 변환해주는 to_categorical() 함수가 존재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터로 잘 표현되었는지 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 훈련 데이터와, 검증 데이터로 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X, labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state = 1993,\n",
    "                                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Keras로 MLP 만들어보기\n",
    "위에서 설명 드렸던 대로 MLP는 입력-은닉-출력 3가지 층으로 이루어져 있습니다.<br>\n",
    "각 층별 퍼셉트론의 수는 제가 임의로 정한 것이니 절대적인 수치가 아닙니다.<br>\n",
    "Keras에는 모델을 구성할 때 순차적 모델(Sequential Model)과 함수형 모델(Functional API Model) 방식으로 크게 2가지가 있습니다.<br>\n",
    "MLP에서는 순차적 모델(Sequential Model)을 사용해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 순차적 모델을 생성합니다.\n",
    "model = models.Sequential()\n",
    "# 입력 층의 차원은 MNIST 이미지가 28*28 = 784 이므로 784로 설정합니다.\n",
    "input_shape = (784,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에서 한가지 주의할 점은 입력 층은 입력 그 자체이지 벡터 연산을 하지 않습니다.\n",
    "# 순차적 모델에 은닉층을 추가합니다. 은닉층은 10개의 퍼셉트론과 relu 활성화 함수, 입력 차원을 넣습니다.\n",
    "model.add(layers.Dense(units = 1024, activation = 'relu', input_shape=input_shape))\n",
    "\n",
    "# 순차적 모델에 출력층을 추가합니다. \n",
    "# 출력 층은 우리가 예측할 0~9의 숫자 개수인 10개의 퍼셉트론과 softmax 활성화 함수를 사용합니다.\n",
    "model.add(layers.Dense(units = 10, activation = 'softmax'))\n",
    "\n",
    "#10개의 숫자 클래스를 분류하는 것이므로 categorical_crossentropy를 loss function으로 사용합니다.\n",
    "#loss를 줄여주는 옵티마이저는 Stochestic Gradient Descent를 사용합니다.\n",
    "model.compile(optimizer='SGD',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 확인\n",
    "실제 입력층은 784개의 퍼셉트론을 가지고 있지만, 표현상의 제약으로 20개의 퍼셉트론으로 표현하였습니다.<br>\n",
    "![MLP_Model](./Images/MLP.png)\n",
    "Keras는 model.summary() 함수를 통해 모델의 각 층을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주의 : GPU로 Keras 또는 Tensorflow를 사용할 때 Video Memory 제한 이슈\n",
    "일반적으로 파이썬에서 데이터를 불러와 작업을 하게되면 우리가 주로 말하는 주기억장치(RAM)에 데이터가 올라온 상태로 작업이 진행됩니다.<br>\n",
    "하지만, GPU로 케라스 또는 텐서플로우를 사용하게되면 램에 있던 데이터를 모두 GPU 전용 RAM인 Video Memory에 데이터가 올라옵니다.<br>\n",
    "그렇기 때문에 케라스와 텐서플로우를 사용할 때 모델이 사용하는 데이터 크기에 상관없이 기본적으로 GPU의 공간을 90%까지 최대로 할당하게 됩니다.<br>\n",
    "이러한 이유로 여러명이 하나의 GPU를 공유하는 경우에 한명이 GPU의 Video Memory를 모두 할당하게되면, 다른 인원은 해당 GPU의 메모리를 할당할 수 없어<br>\n",
    "GPU를 사용할 수 없게됩니다. \n",
    "\n",
    "#### Allow Growth?\n",
    "이런 문제 때문에 GPU를 공유하거나, 하나의 GPU에서 여러 모델을 학습하고 싶을 때 Allow Growth 즉, 필요한 만큼만 메모리를 할당하도록 설정할 수 있습니다.<br>\n",
    "Allow Growth 설정을 통해 GPU의 메모리를 더욱 효율적으로 사용할 수 있습니다.<br>\n",
    "케라스는 GPU로 학습할 때 기본적으로 텐서플로우를 백엔드로 사용하기 때문에 텐서플로우를 불러와 Allow Growth 설정을 해주면됩니다.<br>\n",
    "\n",
    "Allow Growth 설정 전 \n",
    "![Allow_Growth_Before](./Images/Allow_Growth_Before.png)\n",
    "Allow Growth 설정 후 \n",
    "![Allow_Growth_After](./Images/Allow_Growth_After.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 케라스 백 엔드인 텐서플로우의 세션 설정을 불러옵니다.\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "# 텐서플로우의 ConfigProto() 객체에 학습시 적용할 옵션들을 명시적으로 설정할 수 있습니다.\n",
    "config = tf.ConfigProto()\n",
    "# GPU 옵션으로 allow_grouth를 True로 설정합니다.\n",
    "config.gpu_options.allow_growth = True \n",
    "\n",
    "# 텐서플로우는 세션이라는 실행 단위를 가지고 있는데, 해당 세션에 적용할 옵션을 담고있는 Config객체를 전달합니다.\n",
    "sess = tf.Session(config=config)\n",
    "# 설정한 세션을 현재 프로세스에 적용합니다.\n",
    "set_session(sess)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습\n",
    "model.fit() 함수를 사용해 모델을 학습시킬 수 있습니다.<br>\n",
    "\n",
    "### batch_size ?\n",
    "데이터셋이 큰 경우 데이터를 한번에 학습시킬 수 없기 때문에 데이터셋을 쪼개어 학습하게 되는데<br>\n",
    "쪼개어 학습하는 단위를 batch 라고 부릅니다. 예제 코드에서는 batch_size가 512이므로 동시에 512장의 이미지를 학습하는 모델이 됩니다. <br>\n",
    "\n",
    "### epoch ?\n",
    "데이터셋 전체를 한번 학습하는 단위를 epoch(에폭, 에포크) 라고 부릅니다.<br>\n",
    "예제 코드에서는 batch_size가 512이고, epochs가 10 이므로, 동시에 512장의 이미지를 학습하며, 데이터셋 전체를 10번 학습하게 됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_x, train_y, \n",
    "          validation_data = [test_x, test_y],\n",
    "          batch_size=1024,\n",
    "          epochs=10,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Keras로 Deep-Feedforward NN 만들어보기\n",
    "Deep Feedforward Neural Network는 기존의 MLP 보다 조금 더 많은 은닉층을 사용하는 모델입니다.<br>\n",
    "MLP에서도 괜찮은 성능을 보였지만, 조금 더 은닉층을 추가해 손글씨 분류 정확도를 높혀보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 순차적 모델을 생성합니다.\n",
    "model = models.Sequential()\n",
    "# 입력 층의 차원은 MNIST 이미지가 28*28 = 784 이므로 784로 설정합니다.\n",
    "input_shape = (784,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 여기에서 한가지 주의할 점은 입력 층은 입력 그 자체이지 벡터 연산을 하지 않습니다.\n",
    "# 순차적 모델에 3개의 은닉층을 추가합니다. 임의로 퍼셉트론의 수를 늘려보겠습니다.\n",
    "model.add(layers.Dense(units = 128, activation = 'relu', input_shape=input_shape))\n",
    "model.add(layers.Dense(units = 64, activation = 'relu'))\n",
    "model.add(layers.Dense(units = 32, activation = 'relu'))\n",
    "\n",
    "# 순차적 모델에 출력층을 추가합니다. \n",
    "# 출력 층은 우리가 예측할 0~9의 숫자 개수인 10개의 퍼셉트론과 softmax 활성화 함수를 사용합니다.\n",
    "model.add(layers.Dense(units = 10, activation = 'softmax'))\n",
    "\n",
    "#10개의 숫자 클래스를 분류하는 것이므로 categorical_crossentropy를 loss function으로 사용합니다.\n",
    "#loss를 줄여주는 옵티마이저는 Stochastic Gradient Descent를 사용합니다.\n",
    "model.compile(optimizer='SGD',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 확인\n",
    "Keras는 model.summary() 함수를 통해 모델의 각 층을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습\n",
    "model.fit() 함수를 사용해 모델을 학습시킬 수 있습니다.<br>\n",
    "\n",
    "### batch_size ?\n",
    "데이터셋이 큰 경우 데이터를 한번에 학습시킬 수 없기 때문에 데이터셋을 쪼개어 학습하게 되는데<br>\n",
    "쪼개어 학습하는 단위를 batch 라고 부릅니다. 예제 코드에서는 batch_size가 512이므로 동시에 512장의 이미지를 학습하는 모델이 됩니다. <br>\n",
    "\n",
    "### epoch ?\n",
    "데이터셋 전체를 한번 학습하는 단위를 epoch(에폭, 에포크) 라고 부릅니다.<br>\n",
    "예제 코드에서는 batch_size가 512이고, epochs가 10 이므로, 동시에 512장의 이미지를 학습하며, 데이터셋 전체를 10번 학습하게 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 모델에 훈련데이터와, 검증하기 위한 테스트 데이터를 넣어 학습을 시작합니다.\n",
    "# batch_size의 크기만큼 모델이 한번에 훈련 데이터를 학습하며, epchos 만큼 반복하여 데이터셋을 학습합니다.\n",
    "history = model.fit(train_x, train_y, \n",
    "          validation_data = [test_x, test_y],\n",
    "          batch_size=512,\n",
    "          epochs=10,\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 연습문제 : 자유롭게 네트워크를 구성하여 95%의 정확도를 만들어 보세요!!\n",
    "20분간 네트워크를 자유롭게 변경하여 훈련 데이터에 대해 95%의 정확도를 만들어보세요.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 순차적 모델을 생성합니다.\n",
    "model = models.Sequential()\n",
    "# 입력 층의 차원은 MNIST 이미지가 28*28 = 784 이므로 784로 설정합니다.\n",
    "input_shape = (784,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 여기에서 한가지 주의할 점은 입력 층은 입력 그 자체이지 벡터 연산을 하지 않습니다.\n",
    "# 순차적 모델에 3개의 은닉층을 추가합니다. 임의로 퍼셉트론의 수를 늘려보겠습니다.\n",
    "model.add(layers.Dense(units = 128, activation = 'relu', input_shape=input_shape))\n",
    "'''\n",
    "은닉층을 퍼셉트론의 수와 활성화 함수를 변경하며 자유롭게 추가해 보세요.\n",
    "활성화 함수에는 sigmoid, relu, tanh 등이 있습니다.\n",
    "model.add(layers.Dense(units = , activation = ''))\n",
    "'''\n",
    "# 순차적 모델에 출력층을 추가합니다. \n",
    "# 출력 층은 우리가 예측할 0~9의 숫자 개수인 10개의 퍼셉트론과 softmax 활성화 함수를 사용합니다.\n",
    "model.add(layers.Dense(units = 10, activation = 'softmax'))\n",
    "\n",
    "\n",
    "model.compile(\n",
    "             #옵티마이저만 자유롭게 변경해보세요. SGD, adam, adadelta, adagrad, rmsprop등이 있습니다.\n",
    "             optimizer='SGD',\n",
    "              \n",
    "              \n",
    "             #10개의 숫자 클래스를 분류하는 것이므로 categorical_crossentropy를 loss function으로 사용합니다.\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_x, train_y, \n",
    "          validation_data = [test_x, test_y],\n",
    "          batch_size=512,\n",
    "          epochs=10,\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "- MNIST : https://en.wikipedia.org/wiki/MNIST_database ,https://github.com/clintonreece/keras-cloud-ml-engine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
