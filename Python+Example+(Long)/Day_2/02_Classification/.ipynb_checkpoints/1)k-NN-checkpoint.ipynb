{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab02-2\n",
    "============"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Context\n",
    "#### Classification\n",
    "   + k-Nearest Neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# k-Nearest Neighborhood \n",
    "\n",
    "k-Nearest Neighborhood(k-NN) 알고리즘은 클래스 분류문제를 해결하기 위해 사용되는 것으로 k개의 **가장 닮은 데이터**를 확인하여 그 비율 중 가장 높은 것으로 추측하는 알고리즘입니다.\n",
    "\n",
    "![title](Images/K-NN.png)\n",
    "\n",
    "#### 가장 닮은 데이터란?\n",
    "데이터는 일종의 벡터이므로 벡터 사이의 거리를 기반으로 가장 가까운 데이터를 의미하며 일반적으로 유클리드 거리를 이용한다.\n",
    "\n",
    "### 알고리즘 특징\n",
    "1. 별도의 학습이 필요 없다(메모리 기반 분류 알고리즘)\n",
    "2. k의 값에 따른 성능 변화\n",
    "3. 결과가 나오기 까지 시간이 오래걸린다(별도의 모델 생성 X)\n",
    "\n",
    "### 알고리즘 순서\n",
    "1. 학습 데이터의 특징 벡터와 항목 분류명(클래스명)을 저장한다.\n",
    "2. 입력 데이터의 특징 벡터와 거리가 가장 가까운 k개의 데이터를 수집한다.\n",
    "3. 수집된 데이터 중 가장 큰 비율을 가진 항목으로 입력 데이터의 항목 분류명을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from os.path import join\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 살펴보기\n",
    "이번 실습에서 우리가 사용할 데이터는 붓꽃의 꽃받침 길이, 꽃받침 너비, 꽃잎의 길이, 꽃잎의 너비 정보, 3가지 종의 열을 가진 150여개의 행이 존재한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_analysis = pd.read_csv(join('data', 'Iris.csv'))\n",
    "\n",
    "iris_analysis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 클래스 별 데이터의 차이를 알아보기 위해 클래스로 분류하겠습니다.\n",
    "이 데이터에는 setosa, versicolor, virginica 3 종의 붓꽃 종류가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_setosa = iris_analysis.loc[iris_analysis['Species'] == 'Iris-setosa']\n",
    "iris_versicolor = iris_analysis.loc[iris_analysis['Species'] == 'Iris-versicolor']\n",
    "iris_virginica = iris_analysis.loc[iris_analysis['Species'] == 'Iris-virginica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 데이터에 대해 종류별로 Boxplot을 통해 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2)\n",
    "\n",
    "#꽃받침의 길이\n",
    "axs[0, 0].boxplot([iris_setosa['SepalLengthCm'], iris_versicolor['SepalLengthCm'], iris_virginica['SepalLengthCm']],\n",
    "           labels = ['seotosa', 'versicolor', 'virginica'],\n",
    "           meanline = True,\n",
    "           showmeans = True)\n",
    "axs[0, 0].set_title('Sepal length Boxplot')\n",
    "\n",
    "#꽃받침의 너비\n",
    "axs[0, 1].boxplot([iris_setosa['SepalWidthCm'], iris_versicolor['SepalWidthCm'], iris_virginica['SepalWidthCm']],\n",
    "           labels = ['seotosa', 'versicolor', 'virginica'],\n",
    "           meanline = True,\n",
    "           showmeans = True)\n",
    "axs[0, 1].set_title('Sepal width Boxplot')\n",
    "\n",
    "#꽃잎의 길이\n",
    "axs[1, 0].boxplot([iris_setosa['PetalLengthCm'], iris_versicolor['PetalLengthCm'], iris_virginica['PetalLengthCm']],\n",
    "           labels = ['seotosa', 'versicolor', 'virginica'],\n",
    "           meanline = True,\n",
    "           showmeans = True)\n",
    "axs[1, 0].set_title('Petal length Boxplot')\n",
    "\n",
    "#꽃잎의 너비\n",
    "axs[1, 1].boxplot([iris_setosa['PetalWidthCm'], iris_versicolor['PetalWidthCm'], iris_virginica['PetalWidthCm']],\n",
    "           labels = ['seotosa', 'versicolor', 'virginica'],\n",
    "           meanline = True,\n",
    "           showmeans = True)\n",
    "axs[1, 1].set_title('Petal width Boxplot')\n",
    "\n",
    "fig.subplots_adjust(left = 0.01, right = 2, bottom = 0.05, top = 2, hspace = 0.3, wspace = 0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "pd.DataFrame(iris.data, columns=[\"sepal.length\", \"sepal.width\", \"petal.length\", \"petal.width\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 간단한 전처리\n",
    "시각화의 편리함을 위해 우리는 꽃잎의 길이, 꽃잎의 너비 2가지의 데이터를 활용하여 붓꽃의 종을 분류할 계획입니다.\n",
    "\n",
    "별도의 학습이라는 것이 필요 없지만 정확도를 살펴보기 위해 학습용 데이터와 테스트용 데이터를 7:3의 비율로 나누었습니다.\n",
    "\n",
    "그래프의 각 점은 데이터를 의미 하며 붉은색 점이 저희가 분류할 데이터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, prediction_X, y, prediction_y = train_test_split(iris.data[:,2:4], iris.target, test_size=0.3, random_state = 1)\n",
    "plt.scatter(X[:,0],X[:,1],marker='o', c = y)\n",
    "plt.scatter(prediction_X[:,0], prediction_X[:,1],marker='o',c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scratch로 분류 해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_k_points\n",
    "point와 다른 데이터의 유클리드 거리를 구하고 이를 정렬한 후 k개의 점을 반환하는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_k_points(point, k):\n",
    "    euclidean_distances = np.sqrt(np.sum((X - point) ** 2, axis = 1))\n",
    "    return np.argsort(euclidean_distances)[0:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict\n",
    "추정할 점들을 입력받아 각 점들에 get_k_points를 수행하여 가장 가까운 k개의 점을 구하고 구한 점들의 Speecies 값의 총합을 구하고 그 중 가장 높은 값을 가지는 Species를 취해 results에 저장하고 이를 모든 predict_point에 대해 수행하는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(predict_point, k):\n",
    "    results = []\n",
    "    for point in predict_point:\n",
    "        min_distances = get_k_points(point, k)\n",
    "        \n",
    "        Species= []\n",
    "        Species_num = [0, 0, 0]\n",
    "        for close_point in min_distances:\n",
    "            Species.append(y[close_point])\n",
    "            \n",
    "        for temp in Species:\n",
    "            Species_num[temp] += 1\n",
    "        results.append([Species_num.index(max(Species_num))])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Scratch 분류 결과 확인하기\n",
    "추정할 점들에 대한 Species결과를 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(prediction_X, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추정한 결과와 실제 데이터가 일치하는지 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(prediction_X, 15)\n",
    "for i in range(0, prediction_y.size):\n",
    "    print(result[i] == prediction_y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 확률로 나타내어 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "\n",
    "result = predict(prediction_X, 15)\n",
    "for i in range(0, prediction_y.size):\n",
    "    if result[i] == prediction_y[i]:\n",
    "        correct += 1;\n",
    "\n",
    "print(correct / prediction_y.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. sklearn으로 분류해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k 값을 의미하는 변수\n",
    "n_neighbors = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-1. 정확도 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scratch로 작성한 것과 마찬가지로 별도의 voting weight없이 수행하는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = neighbors.KNeighborsClassifier(n_neighbors, weights='uniform')\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"uniform  학습 세트 정확도  : {:.3f}%\".format(clf.score(X, y) * 100))\n",
    "print(\"uniform  테스트 세트 정확도: {:.3f}%\".format(clf.score(prediction_X, prediction_y) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Vote\n",
    "K-NN은 **가장 닮은 데이터**의 클래스가 테스트 데이터의 클래스로 추정합니다. 그렇다면 가장 가까운 데이터와 k번째 가까운 데이터 둘 중 어느 데이터의 클래스가 더 테스트 데이터의 클래스에 가까울까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 sklearn에서 제공하는 **거리**를 voting weight로 설정하여 수행하는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
    "clf2.fit(X, y)\n",
    "\n",
    "print(\"distance 학습 세트 정확도  : {:.3f}%\".format(clf2.score(X, y) * 100))\n",
    "print(\"distance 테스트 세트 정확도: {:.3f}%\".format(clf2.score(prediction_X, prediction_y) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-2. 그래프 출력\n",
    "그래프 시각화를 위한 변수들입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mash를 그릴 때 간격을 결정하는 변수\n",
    "h = .02\n",
    "\n",
    "#cmap_light = Mash에 표시될 3가지 색상\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "#cmap_bold = 학습 데이터를 표시할 3가지 색상\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "#그래프 범위 설정을 위한 변수\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "            edgecolor='k', s=15)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i, weights = 'uniform')\"\n",
    "          % (n_neighbors))\n",
    "Z = clf2.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "            edgecolor='k', s=15)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i, weights = 'distance')\"\n",
    "          % (n_neighbors))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 실습\n",
    "k-NN은 별도의 학습도 없이 굉장히 높은 정확도를 보이고 있습니다. 그렇다면 k-NN은 항상 정확도가 높은 분류방법일까요?\n",
    "\n",
    "Iris의 다른 데이터(꽃받침의 길이, 너비)로 위에서 수행한 k-NN분류를 해보시고 결과를 확인해보세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
