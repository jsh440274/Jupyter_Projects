{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab01-2\n",
    "======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "#### Clustering\n",
    "+ Hierarchical Clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "### 특징\n",
    "Hierarchical Clustering은 거리(Distance) 또는 유사도(Similarity)를 기반으로 클러스터를 형성하는 알고리즘 입니다.<br> \n",
    "k-means Clustering과 다르게 클러스터의 수를 설정해 줄 필요가 없으며, 클러스터 형태를 시각적으로 표현해주는 덴드로그램을 통해 적절한 클러스터의 수를 선택할 수 있습니다.<br>\n",
    "Hierachichal Clustering에는 Bottom-Up 방식의 Agglomerative Method와 Top-Down 방식의 Divisive Method로 나뉩니다.<br>\n",
    "이번 단원에서는 Agglomerative Method를 사용해 실습을 진행합니다.\n",
    "<br><br>Agglomerative Method를 사용한 Hierarchical Clustering 알고리즘은 3가지 단계로 이루어집니다.<br>\n",
    "Step 1. 각 데이터 포인트를 클러스터로 할당합니다. (n개의 클러스터)<br>\n",
    "Step 2. 가까운 클러스터끼리 병합합니다.<br>\n",
    "Step 3. 1개의 클러스터가 될 때까지 반복합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "datapath = join('data','wine.txt')\n",
    "labelpath = join('data','wine_attributes.txt')\n",
    "\n",
    "columns = list()\n",
    "with open(labelpath, 'r') as f:\n",
    "    columns = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 살펴보기\n",
    "이번 챕터에서는 우리가 사용할 데이터는 178개의 행과 14개의 열로 이루어진 와인 데이터를 사용해보려고 합니다.\n",
    "<br>데이터를 살펴보기 쉽게 pandas DataFrame으로 읽습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(datapath, names = columns)\n",
    "del data['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame.head() 함수로 처음 5개의 데이터를 살펴볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "와인 데이터는 3개의 클래스로 이루어진 데이터이므로 3개의 클러스터를 형성해 보도록 하겠습니다.\n",
    "<br>마지막에 이번 예제에서 형성한 클러스터가 잘 형성되었는지를 확인하기 위해 실제 클래스를 변수로 저장해둡니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. 간단한 전처리 및 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dim in data.columns:\n",
    "        data[dim] -= np.min(data[dim])\n",
    "        data[dim] /= np.max(data[dim])\n",
    "        \n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)\n",
    "data = pca.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어떻게 가장 가까운 클러스터를 찾을 수 있을까?\n",
    "이전 수업에서 우리는 거리 측정 방법으로 맨하탄 거리, 유클리디언 거리, 마할라노비스 거리에 대해 배웠었습니다.<br>\n",
    "k-means에서는 각 클러스터의 중심점 간의 거리로 클러스터간 거리를 계산했었습니다.<br> 이번 수업에서는 새로운 클러스터간 거리를 계산하는 방법에 대해 알아보겠습니다.<br>\n",
    "#### 1. Single Linkage - 두 클러스터 내의 가장 가까운 점 사이의 거리 \n",
    "![Single Linkage](./Images/Single.png)\n",
    "#### 2. Complete Linkage - 두 클러스터 내의 가장 먼 점 사이의 거리\n",
    "![Complete Linkage](./Images/Complete.png)\n",
    "#### 3. Average Linkage - 두 클러스터 내의 모든 점 사이의 평균 거리\n",
    "![Average Linkage](./Images/Average.png)\n",
    "이번 예제에서 3개 거리 측정 방식의 결과와 차이점을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 차원 축소 \n",
    "wine 데이터는 13개의 컬럼을 가지고 있고, 하나의 데이터(행)는 13개의 차원으로 이루어진 벡터라고 볼 수 있습니다. <br>\n",
    "13차원은 우리 눈으로 볼 수 있도록 표현하기 어려우므로 아직 배우진 않았지만, PCA를 사용해 2차원으로 줄여 시각화할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 3\n",
    "N = len(data)\n",
    "print('클러스터의 개수는 {}개. 데이터의 개수는 {}개 입니다.'.format(num_clusters, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sklearn으로 클러스터링 해보기\n",
    "### 1. Single Linkage - 두 클러스터 내의 가장 가까운 점 사이의 거리\n",
    "Sklearn의 Bottom-Up Hierarchical Clustering 모델인 AgglomerativeClustering을 사용합니다.<br>\n",
    "linkage를 'single'로 설정하고, 클러스터의 수는 3개로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "single_clustering = AgglomerativeClustering(linkage='single', n_clusters=3).fit(data)\n",
    "single_labels = single_clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1], c=single_labels)\n",
    "plt.title('Sklearn Single Linkage Hierarchical Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Hierarchical Clustering의 자식 노드\n",
    "children = single_clustering.children_\n",
    "\n",
    "# 각 자식 노드간의 거리 정보를 가지고 있지 않기 때문에, 균일하게 그리도록 합니다.\n",
    "distance = np.arange(children.shape[0])\n",
    "\n",
    "# 각 클러스터 단계를 포함한 노드의 수 계산\n",
    "no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "# 덴드로그램을 그리기위한 연결 매트릭스를 생성합니다.\n",
    "linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "# 덴드로그램을 그립니다.\n",
    "dendrogram(linkage_matrix, p = N, labels = single_labels, \n",
    "           show_contracted=True, no_labels = True, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Complete Linkage - 두 클러스터 내의 가장 먼 점 사이의 거리\n",
    "linkage를 'complete'로 설정하고, 클러스터의 수는 3개로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_clustering = AgglomerativeClustering(linkage='complete', \n",
    "                                              n_clusters=3).fit(data)\n",
    "complete_labels = complete_clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1], c=complete_labels)\n",
    "plt.title('Sklearn Complete Linkage Hierarchical Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Hierarchical Clustering의 자식 노드\n",
    "children = complete_clustering.children_\n",
    "\n",
    "# 각 자식 노드간의 거리 정보를 가지고 있지 않기 때문에, 균일하게 그리도록 합니다.\n",
    "distance = np.arange(children.shape[0])\n",
    "\n",
    "# 각 클러스터 단계를 포함한 노드의 수 계산\n",
    "no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "# 덴드로그램을 그리기위한 연결 매트릭스를 생성합니다.\n",
    "linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "# 덴드로그램을 그립니다.\n",
    "dendrogram(linkage_matrix, p = N, labels = single_labels, \n",
    "           show_contracted=True, no_labels = True, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Average Linkage - 두 클러스터 내의 모든 점 사이의 평균 거리\n",
    "linkage를 'average'로 설정하고, 클러스터의 수는 3개로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_clustering = AgglomerativeClustering(linkage='average', \n",
    "                                              n_clusters=3).fit(data)\n",
    "average_labels = average_clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1], c=average_labels)\n",
    "plt.title('Sklearn Average Linkage Hierarchical Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Hierarchical Clustering의 자식 노드\n",
    "children = average_clustering.children_\n",
    "\n",
    "# 각 자식 노드간의 거리 정보를 가지고 있지 않기 때문에, 균일하게 그리도록 합니다.\n",
    "distance = np.arange(children.shape[0])\n",
    "\n",
    "# 각 클러스터 단계를 포함한 노드의 수 계산\n",
    "no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "# 덴드로그램을 그리기위한 연결 매트릭스를 생성합니다.\n",
    "linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "# 덴드로그램을 그립니다.\n",
    "dendrogram(linkage_matrix, p = N, labels = single_labels, \n",
    "           show_contracted=True, no_labels = True, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 클러스터링 결과 비교하기\n",
    "\n",
    "1. Single Linkage\n",
    "    + 두 클러스터 내의 가장 가까운 점을 기준으로 클러스터를 합치기 클러스터 사이의 노이즈에 매우 민감한 특성과 구 형태가 아닌 데이터에 대해 클러스터를 잘 형성한다는 특성이 있습니다.\n",
    "    + wine 데이터는 모든 데이터가 연결되어 있는 듯한 분포를 가지고 있기 때문에, 각 클러스터의 경계가 모호한 노이즈가 많은 형태를 띠고 있습니다. <br>Single Linkage가 구 형태가 아닌 데이터에 대해 클러스터를 잘 형성한다는 특성이 있지만, 이러한 데이터의 경우 Single Linkage 방법을 사용하면 좋은 클러스터를 생성하기 어렵습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1], c=single_labels)\n",
    "plt.title('Sklearn Single Linkage Hierarchical Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Complete Linkage\n",
    "    + 두 클러스터 내에 가장 먼 점을 기준으로 클러스터를 합치기 때문에 클러스터 사이의 노이즈와 이상치에 민감하지 않은 특성이 있습니다.\n",
    "    + 노이즈에 민감하지 않다는 특성을 가진 Complete Linkage가 좋은 성능을 보여주었습니다. \n",
    "3. Average Linkage\n",
    "    + Single Linkage와 Complete Linkage의 중간쯤에 위치한 Average Linkage가 가장 정답에 가까운 클러스터를 형성한 것을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(data[:,0], data[:,1], c=complete_labels)\n",
    "plt.title('Sklearn Complete Linkage Hierarchical Clustering')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(data[:,0], data[:,1], c=average_labels)\n",
    "plt.title('Sklearn Average Linkage Hierarchical Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "- SKlearn PCA - https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "- Sklearn AgglomerativeClustering - https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
